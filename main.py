import base64
import io
import os
import sys
from pathlib import Path
from contextlib import asynccontextmanager

import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼(main.py)ì˜ ë””ë ‰í† ë¦¬ ì ˆëŒ€ ê²½ë¡œë¥¼ êµ¬í•©ë‹ˆë‹¤.
BASE_DIR = Path(__file__).resolve().parent

# ê°€ì¤‘ì¹˜ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
MODEL_PATH = BASE_DIR / "models" / "net_params.pth"

# ì‹œìŠ¤í…œ ê²½ë¡œì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬, ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  models íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
if str(BASE_DIR) not in sys.path:
    sys.path.append(str(BASE_DIR))

# ëª¨ë¸ Import
try:
    # models/ccnet.py íŒŒì¼ ì•ˆì˜ ccnet í´ë˜ìŠ¤(ë˜ëŠ” í•¨ìˆ˜) import
    from models.ccnet import ccnet
except ImportError as e:
    print(f"âŒ [Error] ëª¨ë¸ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print(f"ğŸ‘‰ 'models' í´ë” ì•ˆì— '__init__.py'ì™€ 'ccnet.py'ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

# ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
model = None
transform = None

def make_tensor(x):
    """ëª¨ë¸ ì¶œë ¥ì´ íŠœí”Œì¼ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ(ì„ë² ë”©)ë§Œ ì¶”ì¶œ"""
    if isinstance(x, tuple):
        x = x[0]
    return x

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ì¢…ë£Œ ì‹œ ì •ë¦¬í•˜ëŠ” Lifespan ì´ë²¤íŠ¸
    """
    global model, transform
    
    print(f"ğŸš€ Starting server on {DEVICE}...")
    print(f"ğŸ“‚ Model path: {MODEL_PATH}")
    
    try:
        if not MODEL_PATH.exists():
            raise FileNotFoundError(f"Weight file not found at: {MODEL_PATH}")

        model = ccnet(600, weight=0.8)
        
        
        state_dict = torch.load(MODEL_PATH, map_location=DEVICE)
        model.load_state_dict(state_dict, strict=False)
        
 
        model.to(DEVICE)
        model.eval()
        

        transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.Grayscale(num_output_channels=1),
            transforms.ToTensor(),
        ])
        print("âœ… Model loaded successfully!")
        
    except Exception as e:
        print(f"ğŸ”¥ Critical Error loading model: {e}")
        raise e  

    yield
    print("ğŸ‘‹ Shutting down...")

app = FastAPI(lifespan=lifespan)


class PalmRequest(BaseModel):
    base64_image: str


@app.post("/embed")
def get_embedding(request: PalmRequest): 
    
    if model is None:
        raise HTTPException(status_code=500, detail="Model not initialized")

    try:
       
        try:
            image_data = base64.b64decode(request.base64_image)
        except Exception:
             raise ValueError("Invalid Base64 string")

        img = Image.open(io.BytesIO(image_data))
        

        img_tensor = transform(img).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            output = model(img_tensor)
            

        embedding_tensor = make_tensor(output)
      
        embedding_list = embedding_tensor.squeeze().cpu().numpy().tolist()
        
        return {
            "status": "success",
            "embedding_dim": len(embedding_list),
            "embedding": embedding_list
        }

    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        print(f"Inference Error: {e}")
        raise HTTPException(status_code=500, detail="Inference processing failed")

@app.get("/health")
def health_check():
    return {"status": "ok", "device": DEVICE}